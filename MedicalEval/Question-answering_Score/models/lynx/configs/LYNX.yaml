# Copyright (2023) Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

## Data
image_rdir: "./images/"

# put your test file in jsonl format
test_files: [ "./data/Open_VQA_images.jsonl" ]

# change this prompt for different task
prompt: "User: {question}\nBot:"
# the key must match the vision key in test_files
# if you test Open_VQA_videos.jsonl, need to change to "video"
vision_prompt_dict: "image"
output_prompt_dict: "answer"

data: {
  num_frames: 5,
}


## Model
vision_encoder: 'eva_vit_1b'
video_encoding: 'concate'
add_frame_pos: True


LLM: '/nvme/share/VLP_web_data/Lynx/vicuna-7b-v1.1'
use_flash_attn: False
use_adapter: True
adapter_freq: 2


bridge: 'resampler'
bridge_depth: 3
num_bridge_tokens: 32


## General
use_left_pad: True
lower_text: True
freeze_vit: True
freeze_llm: True
image_res: 420
image_mean: [ 0.48145466, 0.4578275, 0.40821073 ]
image_std: [ 0.26862954, 0.26130258, 0.27577711 ]



## Testing
checkpoint: "/nvme/share/VLP_web_data/Lynx/finetune_lynx.pt"

## infer params
max_input_tokens: 40
batch_size_test: 16
max_new_tokens: 64
min_length: 2
num_beams: 5
length_penalty: -2.0
top_p: 0.9
top_k: 3
no_repeat_ngram_size: 2
apply_lemmatizer: False
use_nucleus_sampling: True
