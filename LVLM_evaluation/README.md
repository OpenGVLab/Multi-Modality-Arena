# LVLM evaluation

The detailed code of our evaluation is provided in this directory.
```
├── README.md
├── utils # functions used to evaluate the models
├── models # the inference code of the models used in our evaluation
├── task_datasets # the class of datasets used in our evaluation
├── scripts # example shell script about evaluation
├── eval.py # main eval function (ocr, vqa, caption, kie, mrr, embod, cls)
├── IdealGPT # utils about using IdealGPT
├── IdealGPT_eval.py # IdealGPT evaluation function
├── ImageNetVC_eval.py # ImageNetVC evaluation function
└── whoops_vqa_bem_eval.py # WHOOPS bem metric evaluation
```